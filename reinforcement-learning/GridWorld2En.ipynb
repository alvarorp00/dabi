{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ReXS2HXKPJV"
   },
   "source": [
    "# GridWorld 2:\n",
    "\n",
    "*GridWorld* is a world in the form of a board widely used as test environment in Reinforcement Learning. This board has several types of cells: initial, free, obstacles, terminal... and now teleportation points! The agents must go from the initial cell to the terminal one avoiding the obstacles and traveling the minimum distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CNiMoNIieMcP"
   },
   "source": [
    "Packages required for *GridWorld 2*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "ktwl5VlqK0bc"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "import time\n",
    "from typing import List\n",
    "from scipy import stats as st\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PFS6yGZrLBgu"
   },
   "source": [
    "Functions to display information: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "4Ilqkq_bKtb1"
   },
   "outputs": [],
   "source": [
    "def printMap(world):\n",
    "  # Shows GridWorld map\n",
    "  m = \"[\"\n",
    "  for i in range(world.size[0]):\n",
    "    for j in range(world.size[1]):\n",
    "      if world.map[(i, j)] == 0: \n",
    "        m += \" O \"\n",
    "      elif world.map[(i, j)] == -1:\n",
    "        m += \" X \" \n",
    "      elif world.map[(i, j)] == 1:\n",
    "        m += \" F \"\n",
    "      elif world.map[(i, j)] == 2:\n",
    "        m += \" T \"\n",
    "    if i == world.size[0] - 1:\n",
    "      m += \"]\\n\"\n",
    "    else:\n",
    "      m += \"\\n\"\n",
    "  print(m)\n",
    "\n",
    "def printPolicy(world, policy):\n",
    "  # Shows policy\n",
    "  p = \"[\"\n",
    "  for i in range(world.size[0]):\n",
    "    for j in range(world.size[1]):\n",
    "      if policy[i][j] == 0:\n",
    "        p += \" ^ \"\n",
    "      elif policy[i][j] == 1:\n",
    "        p += \" V \"\n",
    "      elif policy[i][j] == 2:\n",
    "        p += \" < \"\n",
    "      else:\n",
    "        p += \" > \"\n",
    "    if i == world.size[0] - 1:\n",
    "      p += \"]\\n\" \n",
    "    else:\n",
    "      p += \"\\n\"\n",
    "  print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a1AqevxWLS--"
   },
   "source": [
    "# *World* class: \n",
    "\n",
    "This class stores the information of the world:\n",
    "\n",
    "*   *Map*: Matrix that encodes the world with free cells (0), obstacles (-1) and terminal cells (1)\n",
    "*   *Size*: Vector with the size of the world encoding matrix (width, height)\n",
    "\n",
    "The following data is required to create a world:\n",
    "\n",
    "*   Map size (width, height)\n",
    "*   Terminal cell list\n",
    "*   Obstacle cell list\n",
    "*   Teletransportation\n",
    "\n",
    "Notes:\n",
    "* When the agent falls in an obstacle, it is trapped there forever.\n",
    "* When the agent enters in a teletransportation point, it exits through the other.\n",
    "\n",
    "For instance: \n",
    "\n",
    "w = World((10, 10), [(9, 9)], [(2, 4), (4, 2)], [(0, 2), (9, 7)])\n",
    "\n",
    "Creates a world with 10 rows and 10 columns with a terminal state (9, 9), two obstacles in (2, 4) and (4, 2), and a teletransportation system between (0, 2) and (9, 7).\n",
    "\n",
    "![map2.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "DeVtZvIyLJkK"
   },
   "outputs": [],
   "source": [
    "class World:\n",
    "\n",
    "  def __init__(self, size, terminal, obstacle, hole):\n",
    "    # Creates the world\n",
    "    self.size = size\n",
    "    self.map = {}\n",
    "    for i in range(size[0]):\n",
    "      for j in range(size[1]):\n",
    "        # Free states\n",
    "        self.map[(i, j)] = 0\n",
    "        # Terminal states\n",
    "        for t in terminal:\n",
    "          if i==t[0] and j==t[1]:\n",
    "            self.map[(i, j)] = 1\n",
    "        # Obstacles\n",
    "        for o in obstacle:\n",
    "          if i==o[0] and j==o[1]:\n",
    "            self.map[(i, j)] = -1\n",
    "        # Teletransportation\n",
    "        for h in hole:\n",
    "          if i==h[0] and j==h[1]:\n",
    "            self.map[(i, j)] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9oF3NYsGMbLM"
   },
   "source": [
    "Test for the *World* class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sr1mWV2sMMQo",
    "outputId": "ac87aa09-8193-42b8-91a9-41715d1ea938"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ O  O  T  O  O  O  O  O  O  O \n",
      " O  O  O  O  O  O  O  O  O  O \n",
      " O  O  O  O  X  O  O  O  O  O \n",
      " O  O  O  O  O  O  O  O  O  O \n",
      " O  O  X  O  O  O  O  O  O  O \n",
      " O  O  O  O  O  O  O  O  O  O \n",
      " O  O  O  O  O  O  O  O  O  O \n",
      " O  O  O  O  O  O  O  O  O  O \n",
      " O  O  O  O  O  O  O  O  O  O \n",
      " O  O  O  O  O  O  O  T  O  F ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  w = World((10, 10), [(9, 9)], [(2, 4), (4, 2)], [(0, 2), (9, 7)])\n",
    "  printMap(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWtpE_T-MjAG"
   },
   "source": [
    "# *Agent* class:\n",
    "\n",
    "This class controls the agent that learns by Reinforce Learning in *GridWorld*. \n",
    "\n",
    "The following data is required to create an agent:\n",
    "\n",
    "*   *World*: World of the agent.\n",
    "*   *Initial State*: Initial state of the agent.\n",
    "\n",
    "The following methods are used to control the agent:\n",
    "\n",
    "*   *nextState = move(state, action)*: Moves the agent from *state* to *nextState* applying *action*.\n",
    "*   *reward = reward(nextState)*: Returns the *reward* received by the agent when going to *nextState*.\n",
    "*   *nextState, reward = checkAction(state, action)*: Checks the *nextState* and *reward* when the agent takes the *action* in the *state*. This method do not change the internal state of the agent, so it can be used to sweep the state space.\n",
    "*   *nextState, reward = executeAction(action)*: Executes the *action* in the current state and returns the *nextState* and *reward*. This method changes the internal state of the agent, so it should only be used when the agent travels along the world.\n",
    "\n",
    "Note: You can change some properties of the agent (reward distribution, behavior with obstacles...) to improve the performance of the algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "6Y652V2QMdJp"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "  def __init__(self, world, initialState):\n",
    "    # Create an agent\n",
    "    self.world = world\n",
    "    self.state = np.array(initialState)\n",
    "\n",
    "  def move(self, state, action):\n",
    "    # Manage state transitions\n",
    "    nextState = state + np.array(action)\n",
    "    if nextState[0] < 0:\n",
    "      nextState[0] = 0\n",
    "    elif nextState[0] >= self.world.size[0]:\n",
    "      nextState[0] = self.world.size[0] - 1\n",
    "    if nextState[1] < 0:\n",
    "      nextState[1] = 0\n",
    "    elif nextState[1] >= self.world.size[1]:\n",
    "      nextState[1] = self.world.size[1] - 1\n",
    "    if self.world.map[(nextState[0], nextState[1])] == 2:\n",
    "      aux = nextState\n",
    "      for i in range(self.world.size[0]):\n",
    "        for j in range(self.world.size[1]):\n",
    "          if self.world.map[(i, j)] == 2 and (nextState[0] != i and nextState[1] != j):\n",
    "            aux = np.array([i, j])\n",
    "            nextState = aux\n",
    "    return nextState\n",
    "\n",
    "  def reward(self, nextState):\n",
    "    # Manage rewards\n",
    "    if self.world.map[(nextState[0], nextState[1])] == -1:\n",
    "      # Reward when the agent moves to an obstacle\n",
    "      reward = -1 # ** Try different values **\n",
    "    elif self.world.map[(nextState[0], nextState[1])] == 1:\n",
    "      # Reward when the agent moves to a terminal cell\n",
    "      reward = 1 # ** Try different values **\n",
    "    else:\n",
    "      # Reward when the agent moves to a free cell\n",
    "      reward = 0 # ** Try different values ** \n",
    "    return reward\n",
    "\n",
    "  def checkAction(self, state, action):\n",
    "    # Plan the action\n",
    "    nextState = self.move(state, action)\n",
    "    if self.world.map[(state[0], state[1])] == -1: \n",
    "      nextState = state                            \n",
    "    reward = self.reward(nextState)\n",
    "    return nextState, reward\n",
    "\n",
    "  def executeAction(self, action):\n",
    "    # Plan and execute the action\n",
    "    nextState = self.move(self.state, action)\n",
    "    if self.world.map[(self.state[0], self.state[1])] == -1: \n",
    "      nextState = self.state     \n",
    "    else: \n",
    "      self.state = nextState                                 \n",
    "    reward = self.reward(nextState)\n",
    "    return self.state, reward  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JKLltU2RNdqC"
   },
   "source": [
    "Test for the *Agent* class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MRLAuxrsNCeu",
    "outputId": "64642bd0-5858-42af-d3f8-d8ca46099f51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ O  O  T  O  O  O  O  O  O  O \n",
      " O  O  O  O  O  O  O  O  O  O \n",
      " O  O  O  O  X  O  O  O  O  O \n",
      " O  O  O  O  O  O  O  O  O  O \n",
      " O  O  X  O  O  O  O  O  O  O \n",
      " O  O  O  O  O  O  O  O  O  O \n",
      " O  O  O  O  O  O  O  O  O  O \n",
      " O  O  O  O  O  O  O  O  O  O \n",
      " O  O  O  O  O  O  O  O  O  O \n",
      " O  O  O  O  O  O  O  T  O  F ]\n",
      "\n",
      "(array([0, 1]), 0)\n",
      "(array([9, 7]), 0)\n",
      "(array([9, 8]), 0)\n",
      "(array([9, 9]), 1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  # Create the world\n",
    "  w = World((10, 10), [(9, 9)], [(2, 4), (4, 2)], [(0, 2), (9, 7)])\n",
    "  printMap(w)\n",
    "  # Create the agent\n",
    "  a = Agent(w, (0, 0))\n",
    "  # Move the agent through the main diagonal\n",
    "  for i in range(1, 5):\n",
    "    # Show the sates and rewards\n",
    "    print(a.executeAction((0, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lUVoRTXLNnql"
   },
   "source": [
    "# Class work:\n",
    "\n",
    "In this work you are going to implement the two most common value-based methods in reinforcement learning: SARSA and QLearning. In addition, you are going to test both algorithms in a set of scenarios to check if they work and compare their performance.\n",
    "\n",
    "## Worlds: \n",
    "\n",
    "The following worlds are provided in multiple sizes to test the algorithms: \n",
    "\n",
    "*   World 1: Easy maze that can be solved in zigzag.\n",
    "*   World 2: Random obstacles and useful teletransportation.\n",
    "*   World 3: Random obstacles and bad teletransportation.\n",
    "*   World 4: Hard maze with right and wrong ways.\n",
    "\n",
    "Note: Feel free to use some of these scenarios or create your own scenarios. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dyTZdDSIO65O",
    "outputId": "e4a3b0cd-5fc0-45f5-bf95-e323c6cdb5f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World 1: \n",
      "[ O  X  O  O  O \n",
      " O  X  O  X  O \n",
      " O  X  O  X  O \n",
      " O  X  O  X  O \n",
      " O  O  O  X  F ]\n",
      "\n",
      "World 1: \n",
      "[ O  X  O  O  O  X  O  O  O \n",
      " O  X  O  X  O  X  O  X  O \n",
      " O  X  O  X  O  X  O  X  O \n",
      " O  X  O  X  O  X  O  X  O \n",
      " O  X  O  X  O  X  O  X  O \n",
      " O  X  O  X  O  X  O  X  O \n",
      " O  X  O  X  O  X  O  X  O \n",
      " O  X  O  X  O  X  O  X  O \n",
      " O  O  O  X  O  O  O  X  F ]\n",
      "\n",
      "World 1: \n",
      "[ O  X  O  O  O  X  O  O  O  X  O  O  O  X  O  O  O  X  O  O  O \n",
      " O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O \n",
      " O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O \n",
      " O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O \n",
      " O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O \n",
      " O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O \n",
      " O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O \n",
      " O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O \n",
      " O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O \n",
      " O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O \n",
      " O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O \n",
      " O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O \n",
      " O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O \n",
      " O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O \n",
      " O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O \n",
      " O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O \n",
      " O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O \n",
      " O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O \n",
      " O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O \n",
      " O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O \n",
      " O  O  O  X  O  O  O  X  O  O  O  X  O  O  O  X  O  O  O  X  F ]\n",
      "\n",
      "World 2: \n",
      "[ O  O  O  O  O \n",
      " O  O  O  X  O \n",
      " T  O  X  X  O \n",
      " O  O  O  O  O \n",
      " O  O  T  O  F ]\n",
      "\n",
      "World 2: \n",
      "[ O  O  O  O  O  O  O  O  O  O \n",
      " O  O  O  O  O  O  O  O  O  O \n",
      " O  O  X  O  O  O  O  O  O  O \n",
      " O  T  X  O  O  O  X  O  O  O \n",
      " O  X  O  O  O  O  O  O  O  O \n",
      " O  O  O  O  O  O  O  O  O  O \n",
      " O  O  O  O  O  O  O  O  O  O \n",
      " O  X  O  X  X  O  O  O  O  O \n",
      " O  O  O  O  X  O  T  O  O  O \n",
      " O  O  O  O  O  O  O  O  O  F ]\n",
      "\n",
      "World 2: \n",
      "[ O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O \n",
      " O  O  O  O  O  O  O  O  O  O  O  X  O  O  X  O  O  O  O  O  O \n",
      " O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O \n",
      " O  O  O  O  O  O  O  O  O  X  O  O  O  O  O  O  O  O  O  O  O \n",
      " O  X  O  O  O  O  O  O  O  X  X  O  O  O  X  O  X  O  O  O  O \n",
      " O  O  X  X  O  O  O  O  O  O  X  O  O  O  O  X  O  O  O  O  O \n",
      " O  X  T  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O \n",
      " O  O  O  O  O  O  O  X  O  O  O  O  O  O  O  O  O  O  O  O  O \n",
      " O  O  O  O  O  O  O  X  O  X  O  O  O  X  O  O  O  O  O  O  O \n",
      " O  O  O  X  O  O  O  O  O  O  O  X  X  O  O  O  O  X  X  O  O \n",
      " O  O  O  X  O  O  O  O  O  O  O  O  X  O  O  O  O  O  O  O  O \n",
      " O  O  O  O  O  X  O  O  O  O  O  O  O  O  O  O  X  X  X  O  O \n",
      " O  O  O  O  O  O  O  X  O  O  O  O  O  O  O  O  O  X  O  O  O \n",
      " O  X  O  O  O  O  O  O  O  O  O  O  O  O  O  X  O  O  X  O  O \n",
      " O  O  X  O  O  O  O  X  O  O  O  O  O  X  X  O  O  O  X  O  O \n",
      " O  X  O  O  X  O  O  X  O  X  O  O  O  X  O  O  X  O  O  O  O \n",
      " O  O  O  O  O  O  X  X  O  O  O  O  O  O  O  O  O  O  O  O  O \n",
      " O  O  X  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O \n",
      " O  O  O  O  O  O  O  O  X  X  O  O  O  O  T  O  O  O  O  O  O \n",
      " O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O \n",
      " O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  F ]\n",
      "\n",
      "World 3: \n",
      "[ O  O  O  O  T \n",
      " O  O  O  X  O \n",
      " O  O  O  X  O \n",
      " O  O  O  O  O \n",
      " T  O  O  O  F ]\n",
      "\n",
      "World 3: \n",
      "[ O  O  O  O  O  O  O  O  O  O \n",
      " O  O  O  O  O  O  X  X  T  O \n",
      " O  O  O  O  O  O  O  O  O  O \n",
      " O  O  O  X  O  O  O  O  O  O \n",
      " O  O  O  O  O  O  O  O  O  O \n",
      " O  X  X  X  O  O  X  O  O  O \n",
      " O  O  O  O  O  O  O  O  O  O \n",
      " O  O  X  O  O  O  O  O  O  O \n",
      " O  T  O  O  O  O  O  O  O  O \n",
      " O  O  O  O  O  O  O  O  O  F ]\n",
      "\n",
      "World 3: \n",
      "[ O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O \n",
      " O  O  O  O  O  O  X  O  O  O  O  X  O  O  O  O  O  O  O  O  O \n",
      " O  O  O  O  O  O  O  O  O  O  O  O  X  X  O  O  X  O  T  O  O \n",
      " O  O  O  O  O  O  X  X  O  O  O  O  O  O  O  O  X  O  O  O  O \n",
      " O  O  O  O  O  O  O  X  O  O  O  O  X  O  O  O  O  O  O  O  O \n",
      " O  X  O  X  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O \n",
      " O  O  O  O  O  O  O  O  O  X  O  O  O  X  O  O  O  O  O  O  O \n",
      " O  O  O  O  O  O  X  X  X  X  O  O  O  O  X  O  O  O  O  O  O \n",
      " O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  X  O  O \n",
      " O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  X  O  O \n",
      " O  O  X  O  O  O  O  O  X  X  X  O  O  O  X  X  X  O  O  O  O \n",
      " O  O  O  O  X  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O \n",
      " O  O  O  O  O  O  O  O  O  O  O  X  O  O  O  O  X  O  X  O  O \n",
      " O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  X  O  O  O \n",
      " O  O  O  O  O  O  O  O  O  O  X  X  O  X  O  O  X  O  O  O  O \n",
      " O  O  O  O  O  X  O  X  O  O  X  O  O  O  O  O  X  O  O  O  O \n",
      " O  O  O  O  X  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O \n",
      " O  O  O  X  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O \n",
      " O  O  T  X  X  O  X  O  O  O  O  O  O  O  O  X  O  O  X  O  O \n",
      " O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O \n",
      " O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  F ]\n",
      "\n",
      "World 4: \n",
      "[ O  X  O  X  O  O  O  O  O  X  O  O  O  O  O  X  X  X  O  X  O \n",
      " O  X  O  X  X  X  X  X  O  X  X  X  X  X  O  O  O  X  O  X  O \n",
      " O  X  O  O  O  O  O  O  O  X  O  O  O  X  O  X  X  X  O  X  O \n",
      " O  X  O  X  O  X  O  X  O  X  O  X  O  O  O  O  X  O  O  X  O \n",
      " O  O  O  X  O  X  O  X  X  X  X  X  X  X  X  O  X  O  X  X  O \n",
      " X  X  X  X  O  X  O  O  O  X  O  O  O  O  O  O  X  O  O  O  O \n",
      " O  O  O  O  O  X  X  X  O  X  X  X  X  X  X  O  X  X  O  X  O \n",
      " X  X  X  X  O  X  O  X  O  X  O  O  O  O  O  O  O  O  O  X  O \n",
      " O  O  O  X  O  O  O  X  X  X  O  O  X  X  X  X  X  X  X  X  O \n",
      " O  X  O  X  O  X  O  X  O  O  O  X  X  O  O  O  O  O  O  X  X \n",
      " O  X  O  X  O  X  X  X  O  X  O  X  O  O  X  X  X  X  O  O  O \n",
      " O  X  O  X  O  X  O  O  O  X  O  X  O  X  X  O  O  X  X  X  O \n",
      " O  X  O  O  O  X  X  O  X  X  O  X  O  X  O  O  O  O  O  X  O \n",
      " O  X  X  X  X  X  O  O  X  O  O  O  O  O  O  X  X  X  O  X  O \n",
      " O  O  O  O  X  O  O  X  X  O  X  O  X  X  O  X  O  O  O  X  O \n",
      " X  X  X  O  O  O  X  X  O  O  X  O  O  X  X  X  O  X  X  X  X \n",
      " O  O  X  X  O  X  X  X  X  X  X  X  O  O  O  X  O  X  O  O  O \n",
      " X  O  O  X  O  X  O  O  O  X  O  O  O  X  X  X  O  X  O  X  O \n",
      " X  X  O  O  O  X  X  X  O  X  X  X  O  O  O  X  O  O  O  X  O \n",
      " O  X  X  O  X  X  O  O  O  O  O  X  O  X  X  X  X  X  X  X  O \n",
      " O  O  O  O  O  O  O  X  X  X  O  X  O  O  O  O  O  O  O  X  F ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  \n",
    "  # Word 1 small\n",
    "  obstacles = []\n",
    "  for j in range(0, 4):\n",
    "    obstacles.append((j, 1))\n",
    "  for j in range(1, 5):\n",
    "    obstacles.append((j, 3))\n",
    "  w1p = World((5, 5), [(4, 4)], obstacles, [])\n",
    "  print(\"World 1: \")\n",
    "  printMap(w1p)\n",
    "\n",
    "  # World 1 medium\n",
    "  obstacles = []\n",
    "  for i in [1, 5]:\n",
    "    for j in range(0, 8):\n",
    "      obstacles.append((j, i))\n",
    "  for i in [3, 7]:\n",
    "    for j in range(1, 9):\n",
    "      obstacles.append((j, i))\n",
    "  w1m = World((9, 9), [(8, 8)], obstacles, [])\n",
    "  print(\"World 1: \")\n",
    "  printMap(w1m)\n",
    "\n",
    "  # World 1 big\n",
    "  obstacles = []\n",
    "  for i in [1, 5, 9, 13, 17]:\n",
    "    for j in range(0, 20):\n",
    "      obstacles.append((j, i))\n",
    "  for i in [3, 7, 11, 15, 19]:\n",
    "    for j in range(1, 21):\n",
    "      obstacles.append((j, i))\n",
    "  w1g = World((21, 21), [(20, 20)], obstacles, [])\n",
    "  print(\"World 1: \")\n",
    "  printMap(w1g)\n",
    "\n",
    "  # World 2 small\n",
    "  obstacles = []\n",
    "  for i in range(3):\n",
    "    obstacles.append((np.random.randint(1, 4), np.random.randint(1, 4)))  \n",
    "  w2p = World((5, 5), [(4, 4)], obstacles, [(2, 0), (4, 2)])\n",
    "  print(\"World 2: \")\n",
    "  printMap(w2p)\n",
    "\n",
    "  # World 2 medium\n",
    "  obstacles = []\n",
    "  for i in range(10):\n",
    "    obstacles.append((np.random.randint(1, 9), np.random.randint(1, 9)))  \n",
    "  w2m = World((10, 10), [(9, 9)], obstacles, [(3, 1), (8, 6)])\n",
    "  print(\"World 2: \")\n",
    "  printMap(w2m)\n",
    "\n",
    "  # World 2 big\n",
    "  obstacles = []\n",
    "  for i in range(50):\n",
    "    obstacles.append((np.random.randint(1, 19), np.random.randint(1, 19)))  \n",
    "  w2g = World((21, 21), [(20, 20)], obstacles, [(6, 2), (18, 14)])\n",
    "  print(\"World 2: \")\n",
    "  printMap(w2g)\n",
    "\n",
    "  # World 3 small\n",
    "  obstacles = []\n",
    "  for i in range(3):\n",
    "    obstacles.append((np.random.randint(1, 4), np.random.randint(1, 4)))  \n",
    "  w3p = World((5, 5), [(4, 4)], obstacles, [(4, 0), (0, 4)])\n",
    "  print(\"World 3: \")\n",
    "  printMap(w3p)\n",
    "\n",
    "  # World 3 medium\n",
    "  obstacles = []\n",
    "  for i in range(10):\n",
    "    obstacles.append((np.random.randint(1, 9), np.random.randint(1, 9)))  \n",
    "  w3m = World((10, 10), [(9, 9)], obstacles, [(8, 1), (1, 8)])\n",
    "  print(\"World 3: \")\n",
    "  printMap(w3m)\n",
    "\n",
    "  # World 3 big\n",
    "  obstacles = []\n",
    "  for i in range(50):\n",
    "    obstacles.append((np.random.randint(1, 19), np.random.randint(1, 19)))  \n",
    "  w3g = World((21, 21), [(20, 20)], obstacles, [(18, 2), (2, 18)])\n",
    "  print(\"World 3: \")\n",
    "  printMap(w3g)\n",
    "\n",
    "  # World 4\n",
    "  obstacles = [(0,1),(0,3),(0,9),(0,15),(0,16),(0,17),(0,19),\n",
    "               (1,1),(1,3),(1,4),(1,5),(1,6),(1,7),(1,9),(1,10),(1,11),(1,12),(1,13),(1,17),(1,19),\n",
    "               (2,1),(2,9),(2,13),(2,15),(2,16),(2,17),(2,19),\n",
    "               (3,1),(3,3),(3,5),(3,7),(3,9),(3,11),(3,16),(3,19),\n",
    "               (4,3),(4,5),(4,7),(4,8),(4,9),(4,10),(4,11),(4,12),(4,13),(4,14),(4,16),(4,18),(4,19),\n",
    "               (5,0),(5,1),(5,2),(5,3),(5,5),(5,9),(5,16),\n",
    "               (6,5),(6,6),(6,7),(6,9),(6,10),(6,11),(6,12),(6,13),(6,14),(6,16),(6,17),(6,19),\n",
    "               (7,0),(7,1),(7,2),(7,3),(7,5),(7,7),(7,9),(7,19),\n",
    "               (8,3),(8,7),(8,8),(8,9),(8,12),(8,13),(8,14),(8,15),(8,16),(8,17),(8,18),(8,19),\n",
    "               (9,1),(9,3),(9,5),(9,7),(9,11),(9,12),(9,19),(9,20),\n",
    "               (10,1),(10,3),(10,5),(10,6),(10,7),(10,9),(10,11),(10,14),(10,15),(10,16),(10,17),\n",
    "               (11,1),(11,3),(11,5),(11,9),(11,11),(11,13),(11,14),(11,17),(11,18),(11,19),\n",
    "               (12,1),(12,5),(12,6),(12,8),(12,9),(12,11),(12,13),(12,19),\n",
    "               (13,1),(13,2),(13,3),(13,4),(13,5),(13,8),(13,15),(13,16),(13,17),(13,19),\n",
    "               (14,4),(14,7),(14,8),(14,10),(14,12),(14,13),(14,15),(14,19),\n",
    "               (15,0),(15,1),(15,2),(15,6),(15,7),(15,10),(15,13),(15,14),(15,15),(15,17),(15,18),(15,19),(15,20),\n",
    "               (16,2),(16,3),(16,5),(16,6),(16,7),(16,8),(16,9),(16,10),(16,11),(16,15),(16,17),\n",
    "               (17,0),(17,3),(17,5),(17,9),(17,13),(17,14),(17,15),(17,17),(17,19),\n",
    "               (18,0),(18,1),(18,5),(18,6),(18,7),(18,9),(18,10),(18,11),(18,15),(18,19),\n",
    "               (19,1),(19,2),(19,4),(19,5),(19,11),(19,13),(19,14),(19,15),(19,16),(19,17),(19,18),(19,19),\n",
    "               (20,7),(20,8),(20,9),(20,11),(20,19)]          \n",
    "  print(\"World 4: \")\n",
    "  w4 = World((21, 21), [(20, 20)], obstacles, [])\n",
    "  printMap(w4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Ybv0gq1PX3S"
   },
   "source": [
    "## SARSA:\n",
    "\n",
    "*SARSA* (State-Action-Reward-State-Action) is a value-based method that solves reinforcement learning problems. *SARSA* computes iteratively the value function $Q(S,A)$ and then determines the optimal policy $\\pi$.\n",
    "\n",
    "*SARSA* received this name because of the five variables involved in its update function: current state ($S_t$), current action ($A_t$), current reward ($R_t$), next state ($S_{t+1}$) and next action ($A_{t+1}$). This equation is shown below:\n",
    "\n",
    "\\begin{equation}\n",
    "Q(S_t,A_t) \\leftarrow Q(S_t,A_t) + \\alpha [R_t + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t,A_t)]\n",
    "\\end{equation}\n",
    "\n",
    "Note: $\\alpha$ is the episode length and $\\gamma$ the discount factor.\n",
    "\n",
    "The *SARSA* algorithm follows this scheme: \n",
    "\n",
    "1.   Initialize $Q(S,A)$ for every state and action \n",
    "2.   **Loop** (repeat $3-9$ until convergence):\n",
    "3.   Initialize $S_t$\n",
    "4.   Choose $A_t$ for $S_t$ following the policy $Q(S,A)$\n",
    "5.   **Loop** (repeat $6-9$ until $S_t$ is terminal):\n",
    "6.   Take $A_t$ in $S_t$ and observe $R_t$ and $S_{t+1}$\n",
    "7.   Choose $A_{t+1}$ for $S_{t+1}$ following the policy $Q(S,A)$\n",
    "8.   Update the value $Q(S_t, A_t)$ with the update equation\n",
    "9.   Take $S_{t+1}$ and $A_{t+1}$ as the new $S_t$ and $A_t$\n",
    "\n",
    "The *SARSA* algorithm uses a parameter $\\epsilon \\in (0, 1)$ to search a balance between exploration and exploitation. When it chooses $A_t$ for $S_t$, if a random number is less than $\\epsilon$, it will take a random action; whereas if that number is more than $\\epsilon$, it will take the best action.\n",
    "\n",
    "## Exercise 1:\n",
    "\n",
    "Implement the SARSA algorithm for the previously defined agent and world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "_Note_: Algorithm is defined below, check the class _ReinforcedLearning_ with the common attributes and functions of SARSA and QLearning and the inner implementations of both algorithms, both defined as _Sarsa_ and _QLearning_ classes that extends the _ReinforcedLearning_ class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Below the definition of both algorithms some tests can be found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pktm1-xNRO3R"
   },
   "source": [
    "## Q-Learning:\n",
    "\n",
    "*Q-Learning* is the most common value-based method to solve reinforcement learning problems. This algorithm receives this name from $Q(S,A)$, the value function that updates during its execution. *Q-Learning* is very similar to *SARSA*, but it uses a different update equation:\n",
    "\n",
    "\\begin{equation}\n",
    "Q(S_t,A_t) \\leftarrow Q(S_t,A_t) + \\alpha [R_t + \\gamma max_a{Q(S_{t+1}}, a) - Q(S_t,A_t)]\n",
    "\\end{equation}\n",
    "\n",
    "In this case, the action $A_{t+1}$ in $S_{t+1}$ is taken to exploit the maximum value.\n",
    "\n",
    "The *Q-Learning* algorithm follows this scheme: \n",
    "\n",
    "1.   Initialize $Q(S,A)$ for every state and action \n",
    "2.   **Loop** (repeat $3-8$ until convergence):\n",
    "3.   Initialize $S_t$\n",
    "4.   **Loop** (repeat $6-8$ until $S_t$ is terminal):\n",
    "5.   Choose $A_t$ for $S_t$ following the policy $Q(S,A)$\n",
    "6.   Take $A_t$ in $S_t$ and observe $R_t$ and $S_{t+1}$\n",
    "7.   Update the value $Q(S_t, A_t)$ with the update equation\n",
    "8.   Take $S_{t+1}$ as the new $S_t$\n",
    "\n",
    "## Exercise 2:\n",
    "Implement the Q-Learning algorithm for the previously defined agent and world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "_Note_: Algorithm is defined below, check the class _ReinforcedLearning_ with the common attributes and functions of SARSA and QLearning and the inner implementations of both algorithms, both defined as _Sarsa_ and _QLearning_ classes that extends the _ReinforcedLearning_ class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Below the definition of both algorithms some tests can be found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xM2nw13MSbds"
   },
   "source": [
    "## Analysis:\n",
    "\n",
    "*SARSA* and *Q-Learning* are very similar, they can be applied to the same problems and usually obtain the same solutions. However, the results of both algorithms can be different in certains problems: e.g., in the Cliffworld, SARSA performs safer movements but obtains less value than Q-Learning ([interesting article](https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-navigating-cliffworld-with-sarsa-and-q-learning-cc3c36eb5830)).\n",
    "\n",
    "## Exercise 3:\n",
    "\n",
    "Analyze the results of both algorithms:\n",
    "\n",
    "1.   Performance of SARSA and Q-Learning: Which problems do they solve? When do they find the optimal solution? What are the causes of these results?\n",
    "\n",
    "2.   Differences between SARSA and Q-Learning: Which algorithm solves more scenarios? Which one converges faster? Which one obtains more value?\n",
    "\n",
    "Note: The following variables can be interesting to analyze the results: Diference between resultant and optimal policies, number of iterations required to converge, total return of the problem, and return per episode.\n",
    "\n",
    "3.   Differences with more exploration (higher $\\epsilon$) and more exploitation (lower $\\epsilon$). Which converges faster? Which maximizes return?\n",
    "\n",
    "4.   Differences with other parameters: number of episodes, learning rate ($\\alpha$), and discount factor ($\\gamma$). Which combination gives the best results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note_: The solution to this exercise can be found at the bottom of the document, under the **Comparison between SARSA and QLearning section**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EY9-jH-0UVQY"
   },
   "source": [
    "**Solution**: Comments on the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xuIM_OglUY8M"
   },
   "source": [
    "Note: Feel free to add all the text and code blocks that you need to answer the questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "-8T8MdfDUfWe"
   },
   "outputs": [],
   "source": [
    "# Solution COMMON class\n",
    "\n",
    "class ReinforcedLearning():\n",
    "  def __init__(self, *args, **kwargs):\n",
    "    self._parameters = kwargs.copy()\n",
    "    self.__config = dict({\n",
    "      'ready': False\n",
    "    })\n",
    "\n",
    "  @property\n",
    "  def world(self) -> World:\n",
    "    return self._parameters['world']\n",
    "\n",
    "  @property\n",
    "  def agent(self) -> Agent:\n",
    "    return self._parameters['agent']\n",
    "\n",
    "  @property\n",
    "  def actions(self) -> List:\n",
    "    return self._parameters['actions']\n",
    "\n",
    "  @property\n",
    "  def alpha(self) -> float:\n",
    "    return self._parameters['alpha']\n",
    "\n",
    "  @property\n",
    "  def gamma(self) -> float:\n",
    "    return self._parameters['gamma']\n",
    "\n",
    "  @property\n",
    "  def epsilon(self):\n",
    "    return self._parameters['epsilon']\n",
    "\n",
    "  @property\n",
    "  def q(self) -> np.ndarray:\n",
    "    return self._parameters['q']\n",
    "\n",
    "  @property\n",
    "  def config(self):\n",
    "    return self.__config.copy()\n",
    "\n",
    "  def set_alpha(self, alpha):\n",
    "    self._parameters['alpha'] = alpha\n",
    "\n",
    "  def set_gamma(self, gamma):\n",
    "    self._parameters['gamma'] = gamma\n",
    "\n",
    "  def set_epsilon(self, epsilon):\n",
    "    self._parameters['epsilon'] = epsilon\n",
    "\n",
    "  def configure(self, **kwargs):\n",
    "    self.__config = kwargs.copy()\n",
    "    if 'max_iterations' not in self.__config:\n",
    "      self.__config['max_iterations'] = np.int32(10**3)  # Run for at most this time\n",
    "    if 'max_steps' not in self.__config:\n",
    "      self.__config['max_steps'] = np.int32(10**2)  # Each iteration will run for this steps\n",
    "    if 'theta' not in self.__config:\n",
    "      self.__config['theta'] = np.float32(0.01)  # Difference threshold\n",
    "    if 'init_value' not in self.__config:\n",
    "      self.__config['init_value'] = 0\n",
    "    if 'q' not in self.__config:\n",
    "      self._parameters['q'] = np.ones(\n",
    "        (self.world.size[0], self.world.size[1], len(self.actions))\n",
    "      ) * (self.__config['init_value']) # Q(S,A) = init_value (0, -1, 1, etc)\n",
    "    if 'qs' not in self.__config:\n",
    "      self._parameters['qs'] = [self.q.copy()]  # Q matrix history\n",
    "    self.__config['ready'] = True\n",
    "\n",
    "    __rows, __cols = self.world.size\n",
    "    for __j in range(__rows):\n",
    "      for __i in range(__cols):\n",
    "        if self.world.map[(__j, __i)] == 1:  # Q(Terminal, A) = 0\n",
    "          self._parameters['q'][__j][__i] = np.zeros(shape=(len(self.actions,)))\n",
    "\n",
    "  def _lookup_action(self, action_idx):\n",
    "    return self.actions[action_idx]\n",
    "\n",
    "  def _choose_action(self, state):\n",
    "    \"\"\"\n",
    "    Chooses an action (index) either randomly or maximizing the reward in a given state\n",
    "    :param state: current state where the action will be taken\n",
    "    :return: index of action to be taken\n",
    "    \"\"\"\n",
    "    __rnd = st.uniform.rvs()\n",
    "    if __rnd < self.epsilon:\n",
    "      __action = np.random.choice(np.arange(0, len(self.actions)), size=1)[0]\n",
    "    else:\n",
    "      __action = np.argmax(self._lookup_q(state))\n",
    "    return __action\n",
    "\n",
    "  def _difference(self, qs: List[np.ndarray], epoch):\n",
    "    __diff_p = np.abs(np.subtract(qs[epoch-1], qs[epoch-2]))\n",
    "    __q_max = np.max(np.abs(self.q))\n",
    "    if __q_max > 0:\n",
    "      __diff = np.max(__diff_p) / __q_max\n",
    "    else:\n",
    "      __diff = 1\n",
    "    return __diff\n",
    "\n",
    "  def _lookup_q(self, state, action:int=None):\n",
    "    if action is not None:\n",
    "      return self.q[state[0]][state[1]][action]\n",
    "    else:\n",
    "      return self.q[state[0]][state[1]]\n",
    "\n",
    "  def _set_q(self, state, action, value):\n",
    "    self._parameters['q'][state[0]][state[1]][action] = value\n",
    "\n",
    "  def _update_q(self, state, next_state, action, next_action):\n",
    "    \"\"\"\n",
    "    Performs\n",
    "    \\begin{equation}\n",
    "      Q(S_t,A_t) \\leftarrow Q(S_t,A_t) + \\alpha [R_t + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t,A_t)]$\n",
    "    \\end{equation}\n",
    "    :param state: current state\n",
    "    :param next_state: next state\n",
    "    :param action: current action idx\n",
    "    :param next_action: next action idx\n",
    "    \"\"\"\n",
    "    __curr = self._lookup_q(state, action)  # Q(S_t,A_t)\n",
    "    _, __reward = self.agent.checkAction(state, self._lookup_action(action))\n",
    "    __next = __reward + self.gamma * self._lookup_q(next_state, next_action)  # R_t + \\gamma Q(S_{t+1}, A_{t+1})\n",
    "    __new_value = __curr + self.alpha * (__next - __curr)  # Whole equation\n",
    "    self._set_q(state, action, __new_value)\n",
    "\n",
    "  def run(self):\n",
    "    return NotImplementedError('Method not implemented in abstract class')\n",
    "\n",
    "  @DeprecationWarning\n",
    "  def solve(self):\n",
    "    \"\"\"\n",
    "    Deprecated: use Sarsa.getPolicy and call printPolicy instead\n",
    "    \n",
    "    Returns the best path starting from agent's current state\n",
    "    :return: Tuple(List, Bool) --> (path_array, path_found?)\n",
    "    \"\"\"\n",
    "    __epsilon = self.epsilon\n",
    "    self._parameters['epsilon'] = 0  # always choose the best action\n",
    "    __starter_state = self.agent.state\n",
    "    \n",
    "    __convert = lambda s: (s[0], s[1])  # converts state to tuple so it can be hashed\n",
    "    __path = [__starter_state.tolist()]\n",
    "    __path_set = set()\n",
    "    __path_set.add(__convert(__starter_state))\n",
    "    \n",
    "    __found = True\n",
    "    while True:\n",
    "      __action = self._choose_action(self.agent.state)\n",
    "      __prev_state = self.agent.state\n",
    "      __next_state, __reward = self.agent.executeAction(self._lookup_action(__action))\n",
    "      __path.append(__next_state.tolist())\n",
    "      if __convert(__next_state) in __path_set:  # state was already visited\n",
    "          logging.warning(f'Agent traveling through visited state {__prev_state}. Exiting with no path.')\n",
    "          __found = False\n",
    "          break\n",
    "      __path_set.add(__convert(__next_state))  # add next state to the path\n",
    "      if __reward == 1:\n",
    "        break\n",
    "    self._parameters['epsilon'] = __epsilon  # Restore epsilon value\n",
    "    self.agent.state = __starter_state\n",
    "    return __path, __found\n",
    "\n",
    "  def print_q(self):\n",
    "    __q = self.q.copy()\n",
    "    __rows, __cols = self.world.size\n",
    "    print('[')\n",
    "    for __j in range(__rows):\n",
    "      print(f'  Row {__j} [')\n",
    "      for __i in range(__cols):\n",
    "        print(f'    Col {__i}' + str(__q[__j][__i]))\n",
    "      print('  ]')\n",
    "    print(']')\n",
    "\n",
    "  def get_policy(self):\n",
    "    __policy = np.zeros(shape=self.world.size)\n",
    "    for __i in range(self.world.size[0]):\n",
    "      for __j in range(self.world.size[1]):\n",
    "        __policy[__i][__j] = np.argmax(self.q[__i][__j])\n",
    "    return __policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Solution: SARSA code\n",
    "\n",
    "class Sarsa(ReinforcedLearning):\n",
    "  def __init__(self, *args, **kwargs):\n",
    "    super().__init__(*args, **kwargs)\n",
    "\n",
    "  def run(self):\n",
    "    \"\"\"\n",
    "    Runs SARSA algorithm\n",
    "    \"\"\"\n",
    "    \n",
    "    if self.config['ready'] is False:\n",
    "      logging.critical('Sarsa is not configured. Run sarsa.configure')\n",
    "      return\n",
    "    __max_iterations = self.config['max_iterations']\n",
    "    __max_steps = self.config['max_steps']\n",
    "    __theta = self.config['theta']\n",
    "    __qs = self._parameters['qs']  # Q history --> current: starter Q\n",
    "    __epoch = 0\n",
    "    while True:\n",
    "      # self.agent.state = __starter_state  # Set S_t\n",
    "      __state = self.agent.state\n",
    "      __action = self._choose_action(__state)  # Choose A_t\n",
    "      # print(f'Starter action: {__action}')\n",
    "      __c = 0\n",
    "      __stop = False\n",
    "      while __stop is False:\n",
    "        # print(f'Action: {self._lookup_action(__action)} @ idx.{__action}')\n",
    "        __next_state, __reward = self.agent.checkAction(__state, self._lookup_action(__action))  # Check next state and reward with chosen action --> S_{t+1}, R_t\n",
    "        # print(f'Next state: {__next_state}')\n",
    "        __next_action = self._choose_action(__next_state)  # Choose A_{t+1}\n",
    "        self._update_q(state=__state, next_state=__next_state, action=__action, next_action=__next_action)  # Update Q(S_t, A_t) with the update equation\n",
    "\n",
    "        if __reward == 1:  # Terminal state reached --> exit\n",
    "          break\n",
    "\n",
    "        # __nextState, reward = self.agent.checkAction(__state, self._lookup_action(__action))  # Take S_{t} = S_{t+1} <-- Update current state with the next state\n",
    "        __state = __next_state  # Take S_{t} = S_{t+1}\n",
    "        __action = __next_action  # Take A_{t} = A_{t+1} <-- Update current action with the next action\n",
    "\n",
    "        __stop = __c >= __max_steps  # If we've reached max steps\n",
    "        __c += 1\n",
    "      __qs.append(self.q.copy())\n",
    "      if __theta is not None and __theta > 0:\n",
    "        if self._difference(qs=__qs, epoch=__epoch) < __theta:  # Convergence reached\n",
    "          break\n",
    "      __epoch += 1\n",
    "      if __max_iterations is not None:  # We can choose to iterate until convergence\n",
    "        if __epoch >= __max_iterations:\n",
    "          break\n",
    "    # self.agent.state = __starter_state  # Restore agent's position\n",
    "    # print('SARSA finished')\n",
    "    return __qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Solution QLearning code\n",
    "\n",
    "class QLearning(ReinforcedLearning):\n",
    "  def __init__(self, *args, **kwargs):\n",
    "    super().__init__(*args, **kwargs)\n",
    "\n",
    "  def _choose_next_action(self, state):\n",
    "    \"\"\"\n",
    "    Chooses an action (index) maximizing the reward in a given state\n",
    "    :param state: current state where the action will be taken\n",
    "    :return: index of action to be taken\n",
    "    \"\"\"\n",
    "    return np.argmax(self._lookup_q(state))\n",
    "\n",
    "  def run(self):\n",
    "    \"\"\"\n",
    "    Runs QLearning algorithm\n",
    "    \"\"\"\n",
    "    \n",
    "    if self.config['ready'] is False:\n",
    "      logging.critical('QLearning is not configured. Run qlearning.configure')\n",
    "      return\n",
    "    __max_iterations = self.config['max_iterations']\n",
    "    __max_steps = self.config['max_steps']\n",
    "    __theta = self.config['theta']\n",
    "    __qs = self._parameters['qs']  # Q history --> current: starter Q\n",
    "    __epoch = 0\n",
    "    while True:\n",
    "      __state = self.agent.state  # Set S_t\n",
    "      __c = 0\n",
    "      __stop = False\n",
    "      while __stop is False:\n",
    "        __action = self._choose_action(__state)  # Choose A_t following Q(S,A) policy\n",
    "        # print(f'Action: {self._lookup_action(__action)} @ idx.{__action}')\n",
    "        __next_state, __reward = self.agent.checkAction(__state, self._lookup_action(__action))  # Check next state and reward with chosen action --> S_{t+1}, R_t\n",
    "        # print(f'Next state: {__next_state}')\n",
    "        __next_action = self._choose_next_action(__state)  # Choose A_{t+1} <-- argmax(Q(S_{t+1}, a))\n",
    "        self._update_q(state=__state, next_state=__next_state, action=__action, next_action=__next_action)  # Update Q(S_t, A_t) with the update equation\n",
    "        if __reward == 1:  # Terminal state reached --> exit\n",
    "          break\n",
    "        __state = __next_state\n",
    "        __stop = __c >= __max_steps  # If we've reached max steps\n",
    "        __c += 1\n",
    "      __qs.append(self.q.copy())\n",
    "      if __theta is not None and __theta > 0:\n",
    "        if self._difference(qs=__qs, epoch=__epoch) < __theta:  # Convergence reached\n",
    "          break\n",
    "      __epoch += 1\n",
    "      if __max_iterations is not None:  # We can choose to iterate until convergence\n",
    "        if __epoch >= __max_iterations:\n",
    "          break\n",
    "    # print('QLearning finished')\n",
    "    return __qs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison between SARSA & QLearning\n",
    "\n",
    "Here we will be testing how fast both algorithms converge for the 3 first given maps in their small versions (w1p, w2p and w3p), and we always be starting at (0,0).\n",
    "\n",
    "### Parameters used and range of application\n",
    "\n",
    "For both algorithms and mentioned maps, we have these general parameters:\n",
    "\n",
    "- $\\alpha$: episode length\n",
    "- $\\gamma$: discount\n",
    "- $\\epsilon$: exploration & exploitation balance\n",
    "\n",
    "And another specific parameters of the simulation:\n",
    "\n",
    "- $max\\_iterations$: maximum number of iterations to run if convergence is not reached\n",
    "- $max\\_steps$: maximum number of steps to run in each iteration if a terminal state is not reached (e.g. falling into an obstacle)\n",
    "- $initial\\_value$: initial value of the $\\mathcal{Q}(S_t)$ vector except for any $S_t$ s.t. $S_t \\in Terminal$, as $\\forall S_t \\in \\, Terminal \\rightarrow Q(S_t) = \\overrightarrow{0}$\n",
    "- $\\theta$: convergence criteria, namely if last iteration(s) is(are) not providing further information to the simulation so we've reached an stationary state\n",
    "\n",
    "My implementations of _SARSA_ and _QLearning_ allows $max\\_iterations$ to be set to `None` and run until convergence is reached or $\\theta$ to be set to `None` $0$ and run for all the given $max\\_iterations$ parameter. WARNING: do not set $max\\_iterations$ to `None` and $\\theta$ to `None` or $0$ at the same time as we would be doing an infinite loop.\n",
    "\n",
    "So, for any run of the algorithms we fix the next parameters as follows:\n",
    "\n",
    "| **$max\\_iterations$** | **$max\\_steps$** | **$\\theta$** |\n",
    "|-----------------------|------------------|--------------|\n",
    "| $1e^3$                | $1e^3$              | $1e^{-6}$       |\n",
    "\n",
    "And for the rest, an option is to follow what's told in [Parameters tuning and optimization for\n",
    "Reinforcement Learning algorithms using\n",
    "Evolutionary Computing](https://wouter.caarls.org/files/inciscos2018.pdf). In this case, I've taken these arbitrarily for 1.,2. and 3., although we'll try some variations in 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMON\n",
    "start = (0,0)\n",
    "actions = np.array([(-1,0), (1,0),(0,-1), (0,1)])  # up, down, left, right\n",
    "worlds = [w1p, w2p, w3p]\n",
    "\n",
    "# MANDATORY\n",
    "max_iterations = 1e3\n",
    "max_steps = 1e3\n",
    "theta = 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First comparison: solutions of both algorithms to the small maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to see with frozen parameters $\\alpha=0.01$, $\\gamma=0.9$ and $\\epsilon=0.85$ how both algorithms perform in each of the three small maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa_runs = []\n",
    "sarsa_times = []\n",
    "qlearning_runs = []\n",
    "qlearning_times = []\n",
    "\n",
    "__r1_alpha = 0.01\n",
    "__r1_gamma = 0.9\n",
    "__r1_epsilon = 0.85\n",
    "__initial_value = 0  # initial value of the Q(S,A) except for terminal states\n",
    "\n",
    "__w_idx = 1\n",
    "for world in worlds:\n",
    "\n",
    "  agent = Agent(world, start)\n",
    "  \n",
    "  ### SARSA RUN ###\n",
    "\n",
    "  sarsa = Sarsa(\n",
    "    world=world,\n",
    "    agent=agent,\n",
    "    actions=actions,\n",
    "    alpha=__r1_alpha,\n",
    "    gamma=__r1_gamma,\n",
    "    epsilon=__r1_epsilon\n",
    "  )\n",
    "\n",
    "  sarsa.configure(\n",
    "    max_iterations=max_iterations,\n",
    "    max_steps=max_steps,\n",
    "    theta=theta,\n",
    "    init_value=__initial_value  # initial value to 0\n",
    "  )\n",
    "\n",
    "  __s_start = time.time()\n",
    "  sarsa.run()\n",
    "  __s_end = time.time()\n",
    "  sarsa_runs.append(sarsa)\n",
    "  sarsa_times.append(__s_end -__s_start)\n",
    "\n",
    "  # print('\\n=============| SARSA Policy |============\\n')\n",
    "\n",
    "  # policy = sarsa.get_policy()\n",
    "  # printPolicy(sarsa.world, policy)\n",
    "\n",
    "  # print('\\n=============| ============ |============\\n')\n",
    "\n",
    "  ### QLEARNING RUN ###\n",
    "\n",
    "  qlearning = QLearning(\n",
    "    world=world,\n",
    "    agent=agent,\n",
    "    actions=actions,\n",
    "    alpha=__r1_alpha,\n",
    "    gamma=__r1_gamma,\n",
    "    epsilon=__r1_epsilon\n",
    "  )\n",
    "\n",
    "  qlearning.configure(\n",
    "    max_iterations=max_iterations,\n",
    "    max_steps=max_steps,\n",
    "    theta=theta,\n",
    "    init_value=__initial_value  # initial value to 0\n",
    "  )\n",
    "  __q_start = time.time()\n",
    "  qlearning.run()\n",
    "  __q_end = time.time()\n",
    "  qlearning_runs.append(qlearning)\n",
    "  qlearning_times.append(__q_end - __q_start)\n",
    "\n",
    "  # print('\\n=============| QLearning Policy |============\\n')\n",
    "\n",
    "  # policy = qlearning.get_policy()\n",
    "  # printPolicy(qlearning.world, policy)\n",
    "\n",
    "  # print('\\n=============| ================ |============\\n')\n",
    "  __w_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SARSA\n",
      "Map w1p\n",
      "[ O  X  O  O  O \n",
      " O  X  O  X  O \n",
      " O  X  O  X  O \n",
      " O  X  O  X  O \n",
      " O  O  O  X  F ]\n",
      "\n",
      "\n",
      "=============| SARSA Q |============\n",
      "\n",
      "[\n",
      "  Row 0 [\n",
      "    Col 0[-4.15457149 -3.03595836 -4.0225703  -9.93009781]\n",
      "    Col 1[-10. -10. -10. -10.]\n",
      "    Col 2[0. 0. 0. 0.]\n",
      "    Col 3[0. 0. 0. 0.]\n",
      "    Col 4[0. 0. 0. 0.]\n",
      "  ]\n",
      "  Row 1 [\n",
      "    Col 0[-3.35954405 -1.84098493 -2.49496598 -9.11120595]\n",
      "    Col 1[-10. -10. -10. -10.]\n",
      "    Col 2[ 0.          0.          0.         -0.03944589]\n",
      "    Col 3[-3.85511274 -3.86517063 -3.89932427 -3.86642937]\n",
      "    Col 4[0. 0. 0. 0.]\n",
      "  ]\n",
      "  Row 2 [\n",
      "    Col 0[-1.68427401 -0.67520317 -0.86740381 -7.36991666]\n",
      "    Col 1[-10. -10. -10. -10.]\n",
      "    Col 2[-9.00000000e-05  0.00000000e+00 -1.93200778e-01 -1.00000000e-02]\n",
      "    Col 3[-3.86591464 -3.8847218  -3.86319849 -3.86671099]\n",
      "    Col 4[ 0.          0.         -0.02968843  0.        ]\n",
      "  ]\n",
      "  Row 3 [\n",
      "    Col 0[-0.80452303 -0.04407879 -0.26451565 -4.96465546]\n",
      "    Col 1[-10. -10. -10. -10.]\n",
      "    Col 2[-8.47279801e-04 -3.39278104e-04 -3.90208190e-01  0.00000000e+00]\n",
      "    Col 3[0. 0. 0. 0.]\n",
      "    Col 4[0. 0. 0. 0.]\n",
      "  ]\n",
      "  Row 4 [\n",
      "    Col 0[-0.35498982 -0.03220009 -0.03208206 -0.20079835]\n",
      "    Col 1[-2.6486162  -0.05252432 -0.01701658 -0.00577702]\n",
      "    Col 2[-0.005035   -0.00262808 -0.05349826 -0.35335308]\n",
      "    Col 3[-8.20095751 -8.20526053 -8.20553917 -8.2076321 ]\n",
      "    Col 4[0. 0. 0. 0.]\n",
      "  ]\n",
      "]\n",
      "None\n",
      "\n",
      "=============| SARSA Policy |============\n",
      "\n",
      "[ V  ^  ^  ^  ^ \n",
      " V  ^  ^  ^  ^ \n",
      " V  ^  V  <  ^ \n",
      " V  V  >  ^  ^ \n",
      " <  >  V  ^  ^ ]\n",
      "\n",
      "\n",
      "=============| ============ |============\n",
      "\n",
      "Map w2p\n",
      "[ O  O  O  O  O \n",
      " O  O  O  X  O \n",
      " T  O  X  X  O \n",
      " O  O  O  O  O \n",
      " O  O  T  O  F ]\n",
      "\n",
      "\n",
      "=============| SARSA Q |============\n",
      "\n",
      "[\n",
      "  Row 0 [\n",
      "    Col 0[-0.08673244 -0.1034223  -0.08653739 -0.16556123]\n",
      "    Col 1[-0.10064088 -0.34037713 -0.0771329  -0.39179925]\n",
      "    Col 2[-0.23018899 -1.55274044 -0.12000068 -0.72355054]\n",
      "    Col 3[-0.37515132 -5.67592694 -0.11813929 -0.07265582]\n",
      "    Col 4[-0.0631795  -0.17162347 -0.4963502  -0.06536221]\n",
      "  ]\n",
      "  Row 1 [\n",
      "    Col 0[-0.06893615 -0.10496143 -0.07281706 -0.34180714]\n",
      "    Col 1[-0.07760136 -0.83308286 -0.07105028 -1.40460589]\n",
      "    Col 2[-0.22579221 -6.62678356 -0.2113358  -5.19832546]\n",
      "    Col 3[-10. -10. -10. -10.]\n",
      "    Col 4[-0.01538701 -0.07453239 -2.40871573 -0.06012184]\n",
      "  ]\n",
      "  Row 2 [\n",
      "    Col 0[0. 0. 0. 0.]\n",
      "    Col 1[-0.11422139 -0.06928514 -0.02131702 -5.4809857 ]\n",
      "    Col 2[-10. -10. -10. -10.]\n",
      "    Col 3[-10. -10. -10. -10.]\n",
      "    Col 4[-0.03034967  0.01072851 -1.97585689 -0.05374504]\n",
      "  ]\n",
      "  Row 3 [\n",
      "    Col 0[-0.05580484 -0.00411621 -0.00766417 -0.07309845]\n",
      "    Col 1[-0.48003178 -0.01534294 -0.0087399  -0.84498177]\n",
      "    Col 2[-6.96534354 -0.02002246 -0.1095689  -0.34999579]\n",
      "    Col 3[-5.52057689e+00  1.61444933e-01 -4.98491056e-01  3.17594958e-03]\n",
      "    Col 4[-0.05192357  0.3573884  -0.33452928  0.00574275]\n",
      "  ]\n",
      "  Row 4 [\n",
      "    Col 0[-0.01325408 -0.00518992 -0.00441001 -0.02158064]\n",
      "    Col 1[-0.15593571 -0.02178557 -0.00448971 -0.04502447]\n",
      "    Col 2[-0.80704945 -0.07568465 -0.02063147  0.23646994]\n",
      "    Col 3[-0.8013012   0.14410365 -0.07257969  0.9709204 ]\n",
      "    Col 4[0. 0. 0. 0.]\n",
      "  ]\n",
      "]\n",
      "None\n",
      "\n",
      "=============| SARSA Policy |============\n",
      "\n",
      "[ <  <  <  >  ^ \n",
      " ^  <  <  ^  ^ \n",
      " ^  <  ^  >  V \n",
      " V  <  V  V  V \n",
      " <  <  >  >  ^ ]\n",
      "\n",
      "\n",
      "=============| ============ |============\n",
      "\n",
      "Map w3p\n",
      "[ O  O  O  O  T \n",
      " O  O  O  X  O \n",
      " O  O  O  X  O \n",
      " O  O  O  O  O \n",
      " T  O  O  O  F ]\n",
      "\n",
      "\n",
      "=============| SARSA Q |============\n",
      "\n",
      "[\n",
      "  Row 0 [\n",
      "    Col 0[-0.10816879 -0.11938739 -0.11019664 -0.21875121]\n",
      "    Col 1[-0.14694034 -0.40454229 -0.10155912 -0.51864392]\n",
      "    Col 2[-0.28447699 -1.74405034 -0.15919401 -0.7468902 ]\n",
      "    Col 3[-4.61692833e-01 -5.14785168e+00 -1.11993125e-01  4.01059343e-03]\n",
      "    Col 4[0. 0. 0. 0.]\n",
      "  ]\n",
      "  Row 1 [\n",
      "    Col 0[-0.09300653 -0.09385226 -0.10669243 -0.40285106]\n",
      "    Col 1[-0.1466829  -0.21141719 -0.11181296 -2.08062425]\n",
      "    Col 2[-0.27065423 -1.1901027  -0.28426854 -8.62969519]\n",
      "    Col 3[-10. -10. -10. -10.]\n",
      "    Col 4[ 0.00045438 -0.0034757  -0.39399144 -0.00088758]\n",
      "  ]\n",
      "  Row 2 [\n",
      "    Col 0[-0.10063614 -0.01364365 -0.06828102 -0.33166777]\n",
      "    Col 1[-0.32791068 -0.0556213  -0.06792835 -1.51199015]\n",
      "    Col 2[-1.05859612 -0.2241862  -0.21505752 -8.11172627]\n",
      "    Col 3[-10. -10. -10. -10.]\n",
      "    Col 4[-0.00443355  0.00102522 -0.95617925 -0.00432933]\n",
      "  ]\n",
      "  Row 3 [\n",
      "    Col 0[-0.07191392  0.00497559 -0.01639373 -0.04670887]\n",
      "    Col 1[-0.30875005  0.01542207 -0.01024849 -0.28661428]\n",
      "    Col 2[-1.39289682  0.06468042 -0.0502021  -0.9125626 ]\n",
      "    Col 3[-6.5512474   0.19048908 -0.11442897  0.02646409]\n",
      "    Col 4[-0.02933512  0.3573884  -0.21949564  0.00520048]\n",
      "  ]\n",
      "  Row 4 [\n",
      "    Col 0[-0.01943837  0.00436334  0.00433479  0.01402599]\n",
      "    Col 1[-0.0615688   0.01245924  0.00522401  0.04251929]\n",
      "    Col 2[-0.31125956  0.07922466  0.01316693  0.30158388]\n",
      "    Col 3[-0.89769928  0.19395631  0.04839442  0.98054657]\n",
      "    Col 4[0. 0. 0. 0.]\n",
      "  ]\n",
      "]\n",
      "None\n",
      "\n",
      "=============| SARSA Policy |============\n",
      "\n",
      "[ ^  <  <  >  ^ \n",
      " ^  <  ^  ^  ^ \n",
      " V  V  <  ^  V \n",
      " V  V  V  V  V \n",
      " >  >  >  >  ^ ]\n",
      "\n",
      "\n",
      "=============| ============ |============\n",
      "\n",
      "QLEARNING\n",
      "Map w1p\n",
      "[ O  X  O  O  O \n",
      " O  X  O  X  O \n",
      " O  X  O  X  O \n",
      " O  X  O  X  O \n",
      " O  O  O  X  F ]\n",
      "\n",
      "\n",
      "=============| QLearning Q |============\n",
      "\n",
      "[\n",
      "  Row 0 [\n",
      "    Col 0[ 0.          0.          0.         -9.98899263]\n",
      "    Col 1[-10. -10. -10. -10.]\n",
      "    Col 2[0. 0. 0. 0.]\n",
      "    Col 3[0. 0. 0. 0.]\n",
      "    Col 4[0. 0. 0. 0.]\n",
      "  ]\n",
      "  Row 1 [\n",
      "    Col 0[ 0.          0.          0.         -8.72289006]\n",
      "    Col 1[-10. -10. -10. -10.]\n",
      "    Col 2[0. 0. 0. 0.]\n",
      "    Col 3[0. 0. 0. 0.]\n",
      "    Col 4[0. 0. 0. 0.]\n",
      "  ]\n",
      "  Row 2 [\n",
      "    Col 0[ 0.          0.          0.         -4.83585899]\n",
      "    Col 1[-9.99999962 -9.99999962 -9.99999962 -9.99999962]\n",
      "    Col 2[0. 0. 0. 0.]\n",
      "    Col 3[0. 0. 0. 0.]\n",
      "    Col 4[0. 0. 0. 0.]\n",
      "  ]\n",
      "  Row 3 [\n",
      "    Col 0[ 0.          0.          0.         -2.15111472]\n",
      "    Col 1[-9.99875827 -9.99875669 -9.99875661 -9.9987601 ]\n",
      "    Col 2[ 0.          0.         -0.19877188  0.        ]\n",
      "    Col 3[0. 0. 0. 0.]\n",
      "    Col 4[0. 0. 0. 0.]\n",
      "  ]\n",
      "  Row 4 [\n",
      "    Col 0[ 0.          0.          0.         -0.06211148]\n",
      "    Col 1[-0.64889308  0.          0.          0.        ]\n",
      "    Col 2[ 0.          0.         -0.00499083 -0.03945706]\n",
      "    Col 3[-3.84161587 -3.84378367 -3.82484299 -3.83002712]\n",
      "    Col 4[0. 0. 0. 0.]\n",
      "  ]\n",
      "]\n",
      "None\n",
      "\n",
      "=============| QLearning Policy |============\n",
      "\n",
      "[ ^  ^  ^  ^  ^ \n",
      " ^  ^  ^  ^  ^ \n",
      " ^  V  ^  ^  ^ \n",
      " ^  <  ^  ^  ^ \n",
      " ^  V  ^  <  ^ ]\n",
      "\n",
      "\n",
      "=============| ================ |============\n",
      "\n",
      "Map w2p\n",
      "[ O  O  O  O  O \n",
      " O  O  O  X  O \n",
      " T  O  X  X  O \n",
      " O  O  O  O  O \n",
      " O  O  T  O  F ]\n",
      "\n",
      "\n",
      "=============| QLearning Q |============\n",
      "\n",
      "[\n",
      "  Row 0 [\n",
      "    Col 0[0. 0. 0. 0.]\n",
      "    Col 1[0. 0. 0. 0.]\n",
      "    Col 2[0. 0. 0. 0.]\n",
      "    Col 3[ 0.         -7.31097556  0.          0.        ]\n",
      "    Col 4[0. 0. 0. 0.]\n",
      "  ]\n",
      "  Row 1 [\n",
      "    Col 0[0.         0.39938575 0.17797607 0.        ]\n",
      "    Col 1[0. 0. 0. 0.]\n",
      "    Col 2[ 0.         -6.62515515  0.         -7.05569251]\n",
      "    Col 3[-10. -10. -10. -10.]\n",
      "    Col 4[ 0.          0.         -2.33225529  0.        ]\n",
      "  ]\n",
      "  Row 2 [\n",
      "    Col 0[0. 0. 0. 0.]\n",
      "    Col 1[ 0.00000000e+00  2.96242754e-05  5.87979147e-02 -4.61847179e+00]\n",
      "    Col 2[-10. -10. -10. -10.]\n",
      "    Col 3[-9.99999982 -9.99999982 -9.99999982 -9.99999982]\n",
      "    Col 4[ 0.          0.         -1.33438431  0.        ]\n",
      "  ]\n",
      "  Row 3 [\n",
      "    Col 0[2.08526499e-03 2.39747800e-08 1.37399795e-04 3.23484339e-05]\n",
      "    Col 1[ 4.37820681e-04 -1.15178094e-01  2.50542691e-04 -2.19229123e+00]\n",
      "    Col 2[-7.00717212  0.25630962 -0.01726391  0.05228972]\n",
      "    Col 3[-4.27413984  0.21018389  0.04073848  0.08179095]\n",
      "    Col 4[0.         0.32427095 0.01487793 0.02134818]\n",
      "  ]\n",
      "  Row 4 [\n",
      "    Col 0[1.20315045e-06 1.37393461e-02 1.84776597e-02 1.27312386e-01]\n",
      "    Col 1[-0.7880198   0.14076767  0.02694281  0.4888229 ]\n",
      "    Col 2[0.02220237 0.55909455 0.27214048 0.81293349]\n",
      "    Col 3[0.02891368 0.62744548 0.42707253 0.95564829]\n",
      "    Col 4[0. 0. 0. 0.]\n",
      "  ]\n",
      "]\n",
      "None\n",
      "\n",
      "=============| QLearning Policy |============\n",
      "\n",
      "[ ^  ^  ^  ^  ^ \n",
      " V  ^  ^  ^  ^ \n",
      " ^  <  ^  ^  ^ \n",
      " ^  ^  V  V  V \n",
      " >  >  >  >  ^ ]\n",
      "\n",
      "\n",
      "=============| ================ |============\n",
      "\n",
      "Map w3p\n",
      "[ O  O  O  O  T \n",
      " O  O  O  X  O \n",
      " O  O  O  X  O \n",
      " O  O  O  O  O \n",
      " T  O  O  O  F ]\n",
      "\n",
      "\n",
      "=============| QLearning Q |============\n",
      "\n",
      "[\n",
      "  Row 0 [\n",
      "    Col 0[0. 0. 0. 0.]\n",
      "    Col 1[0. 0. 0. 0.]\n",
      "    Col 2[0. 0. 0. 0.]\n",
      "    Col 3[ 0.         -9.11586759  0.          0.        ]\n",
      "    Col 4[0. 0. 0. 0.]\n",
      "  ]\n",
      "  Row 1 [\n",
      "    Col 0[0. 0. 0. 0.]\n",
      "    Col 1[0. 0. 0. 0.]\n",
      "    Col 2[ 0.          0.          0.         -9.80216953]\n",
      "    Col 3[-10. -10. -10. -10.]\n",
      "    Col 4[0. 0. 0. 0.]\n",
      "  ]\n",
      "  Row 2 [\n",
      "    Col 0[0. 0. 0. 0.]\n",
      "    Col 1[0. 0. 0. 0.]\n",
      "    Col 2[ 0.          0.          0.         -8.89507457]\n",
      "    Col 3[-10. -10. -10. -10.]\n",
      "    Col 4[ 0.          0.         -0.39400096  0.        ]\n",
      "  ]\n",
      "  Row 3 [\n",
      "    Col 0[0. 0. 0. 0.]\n",
      "    Col 1[0. 0. 0. 0.]\n",
      "    Col 2[ 0.         -0.01213741  0.         -1.28021858]\n",
      "    Col 3[-3.27071597  0.02803182 -0.02058651  0.02495933]\n",
      "    Col 4[0.         0.17383138 0.00142831 0.01045183]\n",
      "  ]\n",
      "  Row 4 [\n",
      "    Col 0[0. 0. 0. 0.]\n",
      "    Col 1[ 0.          0.          0.         -0.02224274]\n",
      "    Col 2[-0.14490063  0.0087461  -0.00160197  0.11794269]\n",
      "    Col 3[0.00289236 0.11461166 0.00637499 0.54339025]\n",
      "    Col 4[0. 0. 0. 0.]\n",
      "  ]\n",
      "]\n",
      "None\n",
      "\n",
      "=============| QLearning Policy |============\n",
      "\n",
      "[ ^  ^  ^  ^  ^ \n",
      " ^  ^  ^  ^  ^ \n",
      " ^  ^  ^  ^  ^ \n",
      " ^  ^  ^  V  V \n",
      " ^  ^  >  >  ^ ]\n",
      "\n",
      "\n",
      "=============| ================ |============\n",
      "\n"
     ]
    }
   ],
   "source": [
    "__m_idx=1\n",
    "print('SARSA')\n",
    "for s in sarsa_runs:\n",
    "    print(f'Map w{__m_idx}p')\n",
    "    printMap(worlds[__m_idx-1])\n",
    "    print('\\n=============| SARSA Q |============\\n')\n",
    "    print(s.print_q())\n",
    "    print('\\n=============| SARSA Policy |============\\n')\n",
    "    printPolicy(worlds[__m_idx-1],s.get_policy())\n",
    "    print('\\n=============| ============ |============\\n')\n",
    "    __m_idx+=1\n",
    "\n",
    "__m_idx=1\n",
    "print('QLEARNING')\n",
    "for q in qlearning_runs:\n",
    "    print(f'Map w{__m_idx}p')\n",
    "    printMap(worlds[__m_idx-1])\n",
    "    print('\\n=============| QLearning Q |============\\n')\n",
    "    print(q.print_q())\n",
    "    print('\\n=============| QLearning Policy |============\\n')\n",
    "    printPolicy(worlds[__m_idx-1],q.get_policy())\n",
    "    print('\\n=============| ================ |============\\n')\n",
    "    __m_idx+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what we notice is, despite that it both still require more time running to solve the maps, is that in any small variations due to the randomness of the algorithm ($\\epsilon$ parameter) will cause to discard a path that potentially is the candidate solution, so it needs a lot of runs to converge; the reason that taking an action (possibly random) in the next state that is not good penalizes current state that could be part of the candidate path.\n",
    "\n",
    "None of them have solved any map but they're close two in some of them (check above output data from the prints).\n",
    "\n",
    "This difference is, namely, that SARSA is _on-policy_ and QLearning _off-policy_, so they'll converge to different solutions as we might have intuitively noticed from the results of the Q matrix. SARSA, on the limit, will still use the _on-policy_ based on the $\\epsilon$ factor and still learn, and QLearning will converge to a greedy solution (as if in the limit we had taken $\\epsilon$ to be so close to $0$).\n",
    "\n",
    "Again, code above generates a light solution for the small maps but adjusting the parameters to run for more time will do the thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second comparison: efficiency and maps solved\n",
    "\n",
    "Here we fix the parameters of the algorithms to $\\alpha=0.01$, $\\gamma=0.9$ and $\\epsilon=0.85$. Then we check the execution for convergence of $\\theta=1e^{-9}$, and we measure the efficiency of the algorithm for each map (w1p, w2p & w3p) in terms of _time_ and _fit_ to the solution (so we care about how precise is the idea gathered from the exploration and also about time required to compute it)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we take the time required by both algorithms above in the previous section, as well as the results given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1008x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sarsa_times_array = np.array(sarsa_times)\n",
    "qlearning_times_array = np.array(qlearning_times)\n",
    "\n",
    "fig = plt.figure(figsize=(14, 7))\n",
    "\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "\n",
    "maps = ['w1p', 'w2p', 'w3p']\n",
    "\n",
    "ax1.bar(maps, sarsa_times_array)\n",
    "ax1.set_title('Sarsa Time')\n",
    "ax1.set_xlabel('world')\n",
    "ax1.set_ylabel('time(s)')\n",
    "\n",
    "ax2.bar(maps, qlearning_times_array)\n",
    "ax2.set_title('QLearning Time')\n",
    "ax2.set_xlabel('world')\n",
    "ax2.set_ylabel('time(s)')\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we conclude that the cases that reach the top in seconds might have reached the maximum number of iterations given, and the others might have converged before finding a solution (so we should need to do more tests with different parameters of $\\alpha$ and $\\gamma$ and check if it's converging or it's reaching the maximum number of iterations).\n",
    "\n",
    "One thing is clear and it's that in the third map SARSA converges/finishes before, but in the other two they're similar. The fact that SARSA converges faster is reasonable as it allows to take random action in the next state and not always the best possible path, so QLearning might suffer from convergence problems derived from this as it's speed of exploring the space is lower than SARSA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Comparison between more exploration (higher $\\epsilon$) and more exploitation (lower $\\epsilon$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I'll be using only SARSA algorithm as it's more dependant of $\\epsilon$ than QLearning, since the next action also depends of this parameter.\n",
    "\n",
    "So, for each world of the ones taken (w1p, w2p and w3p) we tried before with an $\\epsilon$ of 0.85. Now, we are going to set it to $0.5$ and $0.1$ with the same parameters of $\\alpha$ and $\\gamma$ taken before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa_eps_variance_runs = []\n",
    "sarsa_eps_variance_times = []\n",
    "\n",
    "r2_alpha = 0.01\n",
    "r2_gamma = 0.9\n",
    "r2_epsilons = [0.5, 0.1]\n",
    "initial_value = 0  # initial value of the Q(S,A) except for terminal states\n",
    "\n",
    "__w_idx = 1\n",
    "for world in worlds:\n",
    "  for epsilon in r2_epsilons:\n",
    "    agent = Agent(world, start)\n",
    "    \n",
    "    ### SARSA RUN ###\n",
    "\n",
    "    sarsa = Sarsa(\n",
    "      world=world,\n",
    "      agent=agent,\n",
    "      actions=actions,\n",
    "      alpha=r2_alpha,\n",
    "      gamma=r2_gamma,\n",
    "      epsilon=epsilon\n",
    "    )\n",
    "\n",
    "    sarsa.configure(\n",
    "      max_iterations=max_iterations,\n",
    "      max_steps=max_steps,\n",
    "      theta=theta,\n",
    "      init_value=initial_value  # initial value to 0\n",
    "    )\n",
    "\n",
    "    __s_start = time.time()\n",
    "    sarsa.run()\n",
    "    __s_end = time.time()\n",
    "    sarsa_eps_variance_runs.append(sarsa)\n",
    "    sarsa_eps_variance_times.append(__s_end -__s_start)\n",
    "\n",
    "    __w_idx += 1\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(len(sarsa_eps_variance_runs))\n",
    "print(len(r2_epsilons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SARSA\n",
      "Map w0p @ EPSILON : 0.5\n",
      "[ O  X  O  O  O \n",
      " O  X  O  X  O \n",
      " O  X  O  X  O \n",
      " O  X  O  X  O \n",
      " O  O  O  X  F ]\n",
      "\n",
      "\n",
      "=============| SARSA Q |============\n",
      "\n",
      "[\n",
      "  Row 0 [\n",
      "    Col 0[-2.21004556 -2.14481859 -2.30281776 -9.39960424]\n",
      "    Col 1[-10. -10. -10. -10.]\n",
      "    Col 2[-8.34785749e-04 -7.03855202e-03 -6.75938974e-01 -3.36516423e-04]\n",
      "    Col 3[-9.40181265e-04 -1.85771473e-01 -3.39053054e-06 -6.86339029e-09]\n",
      "    Col 4[-7.00446387e-09 -6.36665260e-11 -7.78272479e-07 -6.36716826e-11]\n",
      "  ]\n",
      "  Row 1 [\n",
      "    Col 0[-1.86153374 -1.59284016 -1.68575906 -8.67125034]\n",
      "    Col 1[-10. -10. -10. -10.]\n",
      "    Col 2[-0.01650891 -0.02180393 -0.48995273 -0.43795729]\n",
      "    Col 3[-9.28901124 -9.28960158 -9.28986771 -9.28953307]\n",
      "    Col 4[-7.07405845e-09  0.00000000e+00 -7.29162066e-02  0.00000000e+00]\n",
      "  ]\n",
      "  Row 2 [\n",
      "    Col 0[-1.30900124 -1.23391833 -1.28018792 -7.54896547]\n",
      "    Col 1[-10. -10. -10. -10.]\n",
      "    Col 2[-0.01005651 -0.01868855 -1.04012941 -0.60583775]\n",
      "    Col 3[-9.31600599 -9.31550301 -9.31708364 -9.31720381]\n",
      "    Col 4[ 0.          0.         -0.08365482  0.        ]\n",
      "  ]\n",
      "  Row 3 [\n",
      "    Col 0[-0.73598228 -0.40213784 -1.03495568 -7.85405994]\n",
      "    Col 1[-10. -10. -10. -10.]\n",
      "    Col 2[-0.0392448  -0.06751693 -1.79867188 -0.94785825]\n",
      "    Col 3[-9.67284859 -9.67237574 -9.67274595 -9.67236575]\n",
      "    Col 4[0.   0.01 0.   0.  ]\n",
      "  ]\n",
      "  Row 4 [\n",
      "    Col 0[-1.1182792  -0.45251813 -0.4568439  -0.78042443]\n",
      "    Col 1[-5.78577525 -0.27911816 -0.28751972 -0.27999026]\n",
      "    Col 2[-0.13536203 -0.17801624 -0.30173215 -3.29380715]\n",
      "    Col 3[-9.99978011 -9.99978018 -9.99977997 -9.99977999]\n",
      "    Col 4[0. 0. 0. 0.]\n",
      "  ]\n",
      "]\n",
      "None\n",
      "\n",
      "=============| SARSA Policy |============\n",
      "\n",
      "[ V  ^  >  >  V \n",
      " V  ^  ^  ^  V \n",
      " V  ^  ^  V  ^ \n",
      " V  ^  ^  >  V \n",
      " V  V  ^  <  ^ ]\n",
      "\n",
      "\n",
      "=============| ============ |============\n",
      "\n",
      "Map w0p @ EPSILON : 0.1\n",
      "[ O  X  O  O  O \n",
      " O  X  O  X  O \n",
      " O  X  O  X  O \n",
      " O  X  O  X  O \n",
      " O  O  O  X  F ]\n",
      "\n",
      "\n",
      "=============| SARSA Q |============\n",
      "\n",
      "[\n",
      "  Row 0 [\n",
      "    Col 0[-0.54517584 -0.50400174 -0.5405592  -6.09379004]\n",
      "    Col 1[-10. -10. -10. -10.]\n",
      "    Col 2[-0.02767997 -0.02444598 -1.50767179 -0.01364891]\n",
      "    Col 3[-0.01318731 -1.42128743 -0.01338648 -0.00313563]\n",
      "    Col 4[-0.00513068  0.01317581 -0.02512078 -0.00520319]\n",
      "  ]\n",
      "  Row 1 [\n",
      "    Col 0[-0.44856776 -0.39127113 -0.43002762 -5.58620894]\n",
      "    Col 1[-10. -10. -10. -10.]\n",
      "    Col 2[-0.03879346 -0.05516078 -0.89999204 -1.0688835 ]\n",
      "    Col 3[-9.99675226 -9.99674819 -9.99674969 -9.99675062]\n",
      "    Col 4[-1.26503069e-03  9.83798036e-02 -1.00347279e+00 -7.49582474e-04]\n",
      "  ]\n",
      "  Row 2 [\n",
      "    Col 0[-0.46638682 -0.29512682 -0.42718496 -4.75602958]\n",
      "    Col 1[-9.99999995 -9.99999995 -9.99999995 -9.99999995]\n",
      "    Col 2[-0.04680552 -0.05635623 -1.51190826 -1.42372213]\n",
      "    Col 3[-9.9842278  -9.98423    -9.98422274 -9.98423515]\n",
      "    Col 4[ 4.95490199e-04  3.58677834e-01 -1.02792287e+00  2.13444410e-03]\n",
      "  ]\n",
      "  Row 3 [\n",
      "    Col 0[-0.17044842 -0.09649332 -0.30744204 -8.29125524]\n",
      "    Col 1[-10. -10. -10. -10.]\n",
      "    Col 2[-0.08763187 -0.10398529 -1.70721557 -1.10994178]\n",
      "    Col 3[-9.83708597 -9.83719946 -9.83726962 -9.83732795]\n",
      "    Col 4[ 0.00140803  0.76000752 -0.36323714  0.01521089]\n",
      "  ]\n",
      "  Row 4 [\n",
      "    Col 0[-0.37193841 -0.10373592 -0.11694341 -0.21380029]\n",
      "    Col 1[-8.11340971 -0.11971997 -0.09487574 -0.09116986]\n",
      "    Col 2[-0.0706229  -0.09211508 -0.13130865 -2.40625592]\n",
      "    Col 3[-9.98793485 -9.98794135 -9.98793197 -9.98792641]\n",
      "    Col 4[0. 0. 0. 0.]\n",
      "  ]\n",
      "]\n",
      "None\n",
      "\n",
      "=============| SARSA Policy |============\n",
      "\n",
      "[ V  V  >  >  V \n",
      " V  ^  ^  V  V \n",
      " V  >  ^  <  V \n",
      " V  ^  ^  ^  V \n",
      " V  >  ^  >  ^ ]\n",
      "\n",
      "\n",
      "=============| ============ |============\n",
      "\n",
      "Map w1p @ EPSILON : 0.5\n",
      "[ O  O  O  O  O \n",
      " O  O  O  X  O \n",
      " T  O  X  X  O \n",
      " O  O  O  O  O \n",
      " O  O  T  O  F ]\n",
      "\n",
      "\n",
      "=============| SARSA Q |============\n",
      "\n",
      "[\n",
      "  Row 0 [\n",
      "    Col 0[0.09718403 0.22768503 0.10539154 0.02676342]\n",
      "    Col 1[ 0.01006812  0.00855961  0.09093653 -0.00062741]\n",
      "    Col 2[-0.00031155 -0.00127267  0.00317799 -0.01659537]\n",
      "    Col 3[-1.14844174e-03 -6.91212618e-01 -9.61527944e-06  7.52579505e-09]\n",
      "    Col 4[ 4.04566811e-09  2.10502351e-06 -4.02677050e-03 -3.66070045e-05]\n",
      "  ]\n",
      "  Row 1 [\n",
      "    Col 0[0.10184795 0.36444958 0.15669431 0.04232462]\n",
      "    Col 1[ 0.00498016 -0.02934136  0.14234286 -0.0108374 ]\n",
      "    Col 2[ 2.26092036e-05 -3.38242344e-01  3.97408831e-03 -4.76443435e-01]\n",
      "    Col 3[-9.84464841 -9.84450197 -9.84459236 -9.8442662 ]\n",
      "    Col 4[-2.35541845e-06  2.55990257e-04 -8.32457834e-02  0.00000000e+00]\n",
      "  ]\n",
      "  Row 2 [\n",
      "    Col 0[0. 0. 0. 0.]\n",
      "    Col 1[ 0.00155336  0.00122967  0.07729623 -1.10616058]\n",
      "    Col 2[-9.9999926  -9.9999926  -9.99999259 -9.99999261]\n",
      "    Col 3[-9.99006231 -9.99005651 -9.99004808 -9.99004351]\n",
      "    Col 4[ 8.62676419e-07  1.24480817e-02 -9.93528498e-02  3.78735569e-05]\n",
      "  ]\n",
      "  Row 3 [\n",
      "    Col 0[9.03493342e-02 7.83607650e-05 1.10825688e-03 4.96276627e-04]\n",
      "    Col 1[-0.01403625  0.05990628  0.00192803 -0.02314547]\n",
      "    Col 2[-3.15391611  0.24437195  0.00418137  0.04367336]\n",
      "    Col 3[-2.04460938  0.39538797 -0.01271404  0.02530753]\n",
      "    Col 4[ 1.35727897e-04  2.75019664e-01 -7.76861791e-03  1.54156946e-06]\n",
      "  ]\n",
      "  Row 4 [\n",
      "    Col 0[9.43546008e-03 1.45091166e-04 9.54722007e-05 5.93118092e-03]\n",
      "    Col 1[0.00485538 0.04096569 0.00055315 0.31743295]\n",
      "    Col 2[-0.22154446  0.30747246  0.10027058  0.64930292]\n",
      "    Col 3[-0.05153137  0.47289267  0.29096355  0.99983401]\n",
      "    Col 4[0. 0. 0. 0.]\n",
      "  ]\n",
      "]\n",
      "None\n",
      "\n",
      "=============| SARSA Policy |============\n",
      "\n",
      "[ V  <  <  >  V \n",
      " V  <  <  >  V \n",
      " ^  <  <  >  V \n",
      " ^  V  V  V  V \n",
      " ^  >  >  >  ^ ]\n",
      "\n",
      "\n",
      "=============| ============ |============\n",
      "\n",
      "Map w1p @ EPSILON : 0.1\n",
      "[ O  O  O  O  O \n",
      " O  O  O  X  O \n",
      " T  O  X  X  O \n",
      " O  O  O  O  O \n",
      " O  O  T  O  F ]\n",
      "\n",
      "\n",
      "=============| SARSA Q |============\n",
      "\n",
      "[\n",
      "  Row 0 [\n",
      "    Col 0[0. 0. 0. 0.]\n",
      "    Col 1[0. 0. 0. 0.]\n",
      "    Col 2[ 0.          0.          0.         -0.00032697]\n",
      "    Col 3[-9.00000000e-05 -7.52785514e-02  0.00000000e+00  0.00000000e+00]\n",
      "    Col 4[0. 0. 0. 0.]\n",
      "  ]\n",
      "  Row 1 [\n",
      "    Col 0[0. 0. 0. 0.]\n",
      "    Col 1[0. 0. 0. 0.]\n",
      "    Col 2[0. 0. 0. 0.]\n",
      "    Col 3[-3.82531318 -3.83267518 -3.82371009 -3.82798052]\n",
      "    Col 4[0. 0. 0. 0.]\n",
      "  ]\n",
      "  Row 2 [\n",
      "    Col 0[0. 0. 0. 0.]\n",
      "    Col 1[0. 0. 0. 0.]\n",
      "    Col 2[-0.16786959 -0.15853721 -0.17786661 -0.16888225]\n",
      "    Col 3[0. 0. 0. 0.]\n",
      "    Col 4[0. 0. 0. 0.]\n",
      "  ]\n",
      "  Row 3 [\n",
      "    Col 0[0. 0. 0. 0.]\n",
      "    Col 1[0. 0. 0. 0.]\n",
      "    Col 2[-0.01  0.    0.    0.  ]\n",
      "    Col 3[0. 0. 0. 0.]\n",
      "    Col 4[0. 0. 0. 0.]\n",
      "  ]\n",
      "  Row 4 [\n",
      "    Col 0[0. 0. 0. 0.]\n",
      "    Col 1[0. 0. 0. 0.]\n",
      "    Col 2[0. 0. 0. 0.]\n",
      "    Col 3[0. 0. 0. 0.]\n",
      "    Col 4[0. 0. 0. 0.]\n",
      "  ]\n",
      "]\n",
      "None\n",
      "\n",
      "=============| SARSA Policy |============\n",
      "\n",
      "[ ^  ^  ^  <  ^ \n",
      " ^  ^  ^  <  ^ \n",
      " ^  ^  V  ^  ^ \n",
      " ^  ^  V  ^  ^ \n",
      " ^  ^  ^  ^  ^ ]\n",
      "\n",
      "\n",
      "=============| ============ |============\n",
      "\n",
      "Map w2p @ EPSILON : 0.5\n",
      "[ O  O  O  O  T \n",
      " O  O  O  X  O \n",
      " O  O  O  X  O \n",
      " O  O  O  O  O \n",
      " T  O  O  O  F ]\n",
      "\n",
      "\n",
      "=============| SARSA Q |============\n",
      "\n",
      "[\n",
      "  Row 0 [\n",
      "    Col 0[0.00749976 0.02515764 0.00635907 0.00078475]\n",
      "    Col 1[ 0.00028736  0.00050576  0.00619201 -0.00042358]\n",
      "    Col 2[-0.00014768 -0.01615773  0.00029872 -0.0040165 ]\n",
      "    Col 3[-9.57367093e-04 -3.62028112e-01 -1.06759550e-05  4.26481538e-03]\n",
      "    Col 4[0. 0. 0. 0.]\n",
      "  ]\n",
      "  Row 1 [\n",
      "    Col 0[0.00628388 0.04739163 0.01156467 0.00069406]\n",
      "    Col 1[ 0.00042283  0.01772212  0.00293372 -0.01510935]\n",
      "    Col 2[-2.00647003e-04 -1.05538361e-02  4.40201904e-04 -1.05395528e+00]\n",
      "    Col 3[-9.91264817 -9.91277001 -9.91297113 -9.91284366]\n",
      "    Col 4[ 6.92930979e-07  0.00000000e+00 -9.07568888e-02  0.00000000e+00]\n",
      "  ]\n",
      "  Row 2 [\n",
      "    Col 0[0.01113673 0.08596152 0.02435537 0.01924261]\n",
      "    Col 1[ 0.00055881  0.08820452  0.0104654  -0.0407392 ]\n",
      "    Col 2[-0.01378416  0.07520516  0.00233984 -1.55402837]\n",
      "    Col 3[-9.99997546 -9.9999754  -9.99997542 -9.99997542]\n",
      "    Col 4[ 0.          0.00174179 -0.09999799  0.        ]\n",
      "  ]\n",
      "  Row 3 [\n",
      "    Col 0[0.02700872 0.15132928 0.04973723 0.08271125]\n",
      "    Col 1[0.01177572 0.24060754 0.0283592  0.08817381]\n",
      "    Col 2[-0.02711384  0.39050128  0.03993065  0.02312069]\n",
      "    Col 3[-2.66514619  0.42593879  0.03693909  0.01616523]\n",
      "    Col 4[1.61190000e-06 2.06385716e-01 1.79211319e-03 2.02522483e-03]\n",
      "  ]\n",
      "  Row 4 [\n",
      "    Col 0[0.04914695 0.08913857 0.08970611 0.25243494]\n",
      "    Col 1[0.07549407 0.19307856 0.10315828 0.42310899]\n",
      "    Col 2[0.12710171 0.34139777 0.18939695 0.69033114]\n",
      "    Col 3[-0.10241794  0.50830727  0.27814884  0.9998867 ]\n",
      "    Col 4[0. 0. 0. 0.]\n",
      "  ]\n",
      "]\n",
      "None\n",
      "\n",
      "=============| SARSA Policy |============\n",
      "\n",
      "[ V  <  <  >  ^ \n",
      " V  V  <  ^  ^ \n",
      " V  V  V  V  V \n",
      " V  V  V  V  V \n",
      " >  >  >  >  ^ ]\n",
      "\n",
      "\n",
      "=============| ============ |============\n",
      "\n",
      "Map w2p @ EPSILON : 0.1\n",
      "[ O  O  O  O  T \n",
      " O  O  O  X  O \n",
      " O  O  O  X  O \n",
      " O  O  O  O  O \n",
      " T  O  O  O  F ]\n",
      "\n",
      "\n",
      "=============| SARSA Q |============\n",
      "\n",
      "[\n",
      "  Row 0 [\n",
      "    Col 0[-1.09586408e-18  0.00000000e+00  0.00000000e+00 -1.21762675e-16]\n",
      "    Col 1[-1.35291862e-14  0.00000000e+00  0.00000000e+00 -1.51842718e-12]\n",
      "    Col 2[-1.91266346e-08 -1.68714131e-10  0.00000000e+00 -2.08289050e-06]\n",
      "    Col 3[-0.00023377 -0.06263529  0.          0.        ]\n",
      "    Col 4[0. 0. 0. 0.]\n",
      "  ]\n",
      "  Row 1 [\n",
      "    Col 0[-9.86277671e-21  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "    Col 1[-1.24234951e-16 -7.21710000e-09  0.00000000e+00  0.00000000e+00]\n",
      "    Col 2[-1.87460145e-08  0.00000000e+00  0.00000000e+00 -5.55048277e-02]\n",
      "    Col 3[-4.89572877 -4.90341309 -4.89736431 -4.89429363]\n",
      "    Col 4[0. 0. 0. 0.]\n",
      "  ]\n",
      "  Row 2 [\n",
      "    Col 0[-8.87649904e-23  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "    Col 1[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 -1.57188438e-06]\n",
      "    Col 2[-8.732691e-05  0.000000e+00  0.000000e+00  0.000000e+00]\n",
      "    Col 3[-1.8837709  -1.88125602 -1.8739133  -1.89372894]\n",
      "    Col 4[0. 0. 0. 0.]\n",
      "  ]\n",
      "  Row 3 [\n",
      "    Col 0[-7.98884913e-25  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "    Col 1[-1.41469594e-08  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "    Col 2[-8.0190e-07 -5.9049e-13  0.0000e+00  0.0000e+00]\n",
      "    Col 3[-0.01  0.    0.    0.  ]\n",
      "    Col 4[0. 0. 0. 0.]\n",
      "  ]\n",
      "  Row 4 [\n",
      "    Col 0[-7.04688393e-27 -6.53633111e-29 -1.28125162e-28  0.00000000e+00]\n",
      "    Col 1[-1.27322635e-10 -1.15747850e-12 -5.82387102e-31  0.00000000e+00]\n",
      "    Col 2[-7.21710e-09 -6.49539e-11  0.00000e+00  0.00000e+00]\n",
      "    Col 3[0.   0.   0.   0.01]\n",
      "    Col 4[0. 0. 0. 0.]\n",
      "  ]\n",
      "]\n",
      "None\n",
      "\n",
      "=============| SARSA Policy |============\n",
      "\n",
      "[ V  V  <  <  ^ \n",
      " V  <  V  >  ^ \n",
      " V  ^  V  <  ^ \n",
      " V  V  <  V  ^ \n",
      " >  >  <  >  ^ ]\n",
      "\n",
      "\n",
      "=============| ============ |============\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('SARSA')\n",
    "for __m_idx in range(len(worlds)): # 0,1,2\n",
    "    for __e_idx in range(len(r2_epsilons)): # 0,1\n",
    "        s = sarsa_eps_variance_runs[2*__m_idx + __e_idx]\n",
    "        print(f'Map w{__m_idx}p @ EPSILON : {r2_epsilons[__e_idx]}')\n",
    "        printMap(worlds[__m_idx])\n",
    "        print('\\n=============| SARSA Q |============\\n')\n",
    "        print(s.print_q())\n",
    "        print('\\n=============| SARSA Policy |============\\n')\n",
    "        printPolicy(worlds[__m_idx],s.get_policy())\n",
    "        print('\\n=============| ============ |============\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1008x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sarsa_times_array = np.array(sarsa_eps_variance_times)\n",
    "\n",
    "fig = plt.figure(figsize=(14, 7))\n",
    "\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "\n",
    "maps5 = [r'w1p@$\\epsilon$=0.5', r'w2p@$\\epsilon$=0.5', r'w3p@$\\epsilon$=0.5']\n",
    "maps1 = [r'w1p@$\\epsilon$=0.1', r'w2p@$\\epsilon$=0.1', r'w3p@$\\epsilon$=0.1']\n",
    "\n",
    "ax1.bar(maps5, sarsa_times_array[:3])\n",
    "ax1.set_title(r'Sarsa Time @ $\\epsilon$=.5')\n",
    "ax1.set_xlabel('world')\n",
    "ax1.set_ylabel('time(s)')\n",
    "\n",
    "ax2.bar(maps1, sarsa_times_array[3:])\n",
    "ax2.set_title(r'Sarsa Time @ $\\epsilon$=.5')\n",
    "ax2.set_xlabel('world')\n",
    "ax2.set_ylabel('time(s)')\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa_runs = []\n",
    "sarsa_times = []\n",
    "\n",
    "__r1_alpha = 0.01\n",
    "__r1_gamma = 0.9\n",
    "__r1_epsilon = 0.85\n",
    "__initial_value = 0  # initial value of the Q(S,A) except for terminal states\n",
    "\n",
    "__w_idx = 1\n",
    "for world in worlds:\n",
    "  print(f'\\n\\tRunning world w{__w_idx}p\\n')\n",
    "\n",
    "  printMap(world=world)\n",
    "\n",
    "  agent = Agent(world, start)\n",
    "  \n",
    "  ### SARSA RUN ###\n",
    "\n",
    "  sarsa = Sarsa(\n",
    "    world=world,\n",
    "    agent=agent,\n",
    "    actions=actions,\n",
    "    alpha=__r1_alpha,\n",
    "    gamma=__r1_gamma,\n",
    "    epsilon=__r1_epsilon\n",
    "  )\n",
    "\n",
    "  sarsa.configure(\n",
    "    max_iterations=max_iterations,\n",
    "    max_steps=max_steps,\n",
    "    theta=theta,\n",
    "    init_value=__initial_value  # initial value to 0\n",
    "  )\n",
    "\n",
    "  __s_start = time.time()\n",
    "  sarsa.run()\n",
    "  __s_end = time.time()\n",
    "  sarsa_runs.append(sarsa)\n",
    "  sarsa_times.append(__s_end - -__s_start)\n",
    "\n",
    "  __w_idx += 1\n",
    "\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "sim_env",
   "language": "python",
   "name": "sim_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
